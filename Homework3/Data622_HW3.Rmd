---
title: "Data 622 - Homework #3 (Group 2)"
author: "Mengqin Cai, Zhi Ying Chen, Donny Lofland, Grace Han, Zach Alexander"
date: "4/5/2021"
output: 
  html_document:
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float: true
    code_folding: hide
  pdf_document:
    extra_dependencies: ["geometry", "multicol", "multirow", "xcolor"]
#  md_document:
#    variant: markdown_github
#  pdf_document:
#        extra_dependencies: ["geometry", "multicol", "multirow"]
#theme: lumen
#number_sections: yes
#toc_depth: 3
---

Source Code: [https://github.com/djlofland/DATA622_S2021_Group2/tree/master/Homework3](https://github.com/djlofland/DATA622_S2021_Group2/tree/master/Homework3)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r, include=FALSE}
require(palmerpenguins)
require(dplyr)
require(ggplot2)
require(caret)
require(pROC)
require(GGally)
require(tidyr)
require(Amelia)
require(VIM)
require(MASS)
require(psych)
require(class)
require(tree)
require(rpart)
require(rpart.plot)
require(randomForest)
require(knitr)
require(kableExtra)
require(naniar)
require(mice)
require(gbm)
require(xgboost)
library(skimr)
```


***

## **Part 1: KNN on the Penguins dataset**

***

**Please use the K-nearest neighbor (KNN) algorithm to predict the species variable. Please be sure to walk through the steps you took. (40 points)**

Similar to past assignments when using the Palmer Penguins dataset, we'll first do some quick exploratory analysis to examine the different features available.  

### Load Data

We can see below that there are four continuous variables: `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. Additionally, there are a few categorical variables: `island`, `sex`, and `year`.  

```{r, echo=FALSE}
# Load data
penguins <- data.frame(penguins)

# Display first few rows for inspection
kable(head(penguins)) %>% 
  kable_styling(bootstrap_options = "basic")
```

### Feature Plots

For the continuous variables, we can examine the distributions, broken out by the target variable, `species`.  

```{r, message=FALSE, warning=FALSE, fig.width=12, fig.height=10, echo=FALSE}
# Generate feature pair plots
penguins %>%
  dplyr::select(species, body_mass_g, bill_length_mm, bill_depth_mm, flipper_length_mm) %>%
  ggpairs(aes(color = species, alpha = 0.9))
```

By separating our distributions by our target variable, `species`, we can see that many feature interactions show clustering between Adelie and Chinstrap penguins (red and green), while Gentoo penguins tend to contrast the other two species for most interactions. This is also confirmed by most of the single-variable distributions split out by species in the plots below. Except for the distribution of `bill_length_mm`, distributions for `body_mass_g`, `bill_depth_mm`, and `flipper_length_mm` all show there to be overlapping distributions between Adelie and Chinstrap penguin species.  

Next, we'll do some data tidying to get our data set ready for our KNN model. `year` reflects the date/time of recording and requires some additional thought.  If we hypothesize or expect that variability in penguin features could have a time-dependent effect, then `year` could be important.  For example, if penguin food supplies vary year by year and this impacts growth or yearly temperature affects penguin development, `year` could matter.  If we thought global warming might impact penguins’ physical development or other natural disasters might have impacted growth in specific years, `year` might matter.  That said, in this situation, *because we don't have any reason to expect a time-dependent effect*, `year` probably doesn't add any explanatory power to our models, and we can remove it from our data set.  

```{r}
# remove year column
penguins <- penguins %>% 
  dplyr::select(-year)
```

### Missing Values

Additionally, when exploring missing values, we can see the following:  

```{r warning=FALSE}
# Plot missing values per feature
penguins %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  pivot_longer(cols = 1:7, names_to = 'columns', values_to = 'NA_count') %>%
  arrange(desc(NA_count)) %>%
  ggplot(aes(y = columns, x = NA_count)) + geom_col(fill = 'deepskyblue4') +
    geom_label(aes(label = NA_count)) +
    theme_minimal() +
    labs(title = 'Count of missing values in penguins dataset') +
    theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# Use nanair package to plot missing value patterns
gg_miss_upset(penguins)
```

We can see that 11 individuals have at least one missing data point. The missing data points are clustered - two penguin records are missing `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`, and `sex`.  Nine penguin records are missing just the `sex` data point.  Since we cannot do much with the two records missing most of their data, we will drop these records.  For the records missing just `sex`, while we could try kNN imputation, we don't know enough about gender differences to know if that's a safe operation or whether it would introduce bias.  Therefore, we'll drop these from our data set as well.  

```{r}
# Remove rows with missing values
penguins <- na.omit(penguins)
```

Looking at our dataset, we see that missing values are no longer an issue; however, we still have some additional cleanup to perform on our dataset below before we proceed with kNN modeling.   

```{r}
# Display data from summary
skim(penguins)
```

When setting up for modeling, we typically separate our features from our target to have an `x` and `y`.  With this in mind, we remove `species` from the main dataframe and create a new dataframe, `species_actual`, to hold `species`.  Note that the dataframes must maintain their same order, and we cannot do any operations that might alter dataframe length.  

```{r}
# New dataframe just holding the target
species_actual <- penguins %>% 
  dplyr::select(species)

# Remove the target column so we just have features in the dataframe
penguins <- penguins %>% 
  dplyr::select(-species)
```

Additionally, we can see below in each of our continuous variables’ distributions that the scales are inconsistent across features. For better model performance, we’ll want to standardize each of our features.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# generate a feature pair plot (without species breakout)
penguins %>%
  dplyr::select(body_mass_g, bill_length_mm, bill_depth_mm, flipper_length_mm) %>%
  ggpairs()
```

We normalize our continuous features using a simple z-score standardization.

```{r}
# z-score scale continuous features
penguins[, c("bill_length_mm", 
             "bill_depth_mm", 
             "flipper_length_mm", 
             "body_mass_g")] <- scale(penguins[, c("bill_length_mm", 
                                                   "bill_depth_mm", 
                                                   "flipper_length_mm", 
                                                   "body_mass_g")])
```

As you can see below, after the z-score standardization, the scaling on the x and y-axis is now more consistent across our features.

```{r, message=FALSE, warning=FALSE}
# pair plot to inpect features after scaling
penguins %>%
  dplyr::select(body_mass_g, bill_length_mm, bill_depth_mm, flipper_length_mm) %>%
  ggpairs()
```

Although this may help our KNN model, we’ll have to be careful about interpretability later on! 

Finally, we’ll want to dummy code our categorical `island` and `sex` variables.  This process will create new columns for each value and assign a 0 or 1.  Note that dummy encoding typically drops one value which becomes the baseline.  So if we have a categorical feature with five unique values, we will have four columns.  If all columns are 0, that represents the reference value.  This helps reduce the number of necessary columns.  With dummy columns in place, we need to remove our original `island` and `sex` variable from this dataset. 

```{r}
# Dummy code the island feature - this creates a new column for each island (0 || 1)
island_dcode <- as.data.frame(dummy.code(penguins$island))
penguins <- cbind(penguins, island_dcode)

# remove the original island factor columns - we will rely on the new dummy columns
penguins <- penguins %>% 
  dplyr::select(-island)

# dummy code the sex feature - since there are only 2 states, we will only be adding a single column (0 || 1)
penguins$sex <- dummy.code(penguins$sex)

```

Our data set is ready for our KNN model.  

```{r, warning=FALSE, message=FALSE}
# quick inspect of dataframe
kable(head(penguins)) %>% 
  kable_styling(bootstrap_options = "basic")
```

*** 

### Splitting into a Training/Test set

One problem during the modeling process is overfitting.  In this situation, a model appears to have good predictive power, but it has “memorized” the training data and doesn’t generalize with new data.  So, we train with one dataset and evaluate with a separate dataset not seen during training.  It is common to randomly separate the original dataset into 80% / 20%, with 80% used for training.  Since our target variable is in a separate dataframe, we apply the same separation to it as well.

```{r}
# we use the fixed random seed for reproducibility
set.seed(123)

# use caret to split out training vs test dataframes
trainingRows <- createDataPartition(as.matrix(species_actual$species), p=0.8, list=FALSE)

train_penguins <- penguins[trainingRows,]
test_penguins <- penguins[-trainingRows,]

# also split out the target dataframes
species_actual_train <- species_actual$species[trainingRows]
species_actual_test <- species_actual$species[-trainingRows]
```

***

### Fitting our kNN Model  

With our data split accordingly, we'll need to identify the appropriate number for `k`. `k` represents the number of nearby neighbors used to determine the sample data point class.   A rule of thumb has us start with the square root of the number of observations (i.e., rows or records).  This varies depending on the number of records, and typically this rule of thumb leads to slightly higher than optimal values, but it’s a good starting point.  Later we will use the `caret` package to help find the optimal value for `k`.  

```{r}
# initial k
sqrt(nrow(train_penguins))
```

We can see above, that it is approximately 16. Therefore, we’ll perform our initial kNN model with $k=16$.

```{r}
set.seed(123) 

# kNN with 16 clusters
k16 <- knn(train_penguins, 
           test_penguins, 
           cl=species_actual_train, 
           k=16)

confusionMatrix(k16, species_actual_test)

#misClassError <- mean(k16 != species_actual_test)
#paste0('The number of misclassified penguins with 16 neighbors is: ', misClassError)
#table(k16, species_actual_test)
```

Our resulting confusion matrix shows the kNN predicted `species` compared to the actual `species` designations. We can see that $k=16$ performed well on our test set with an overall $Accuracy=0.9662$ and very good `Sensitivity` and `Specificity` for each class between 0.9231 and 1.00.  This tells us that there were very few false positives and negatives.  While this model performed exceptionally well, our choice of $k=16$ might be sub-optimal.  Next, we will leverage `caret:::train()` to help us find the optimal value for `k` thru testing.  

### Determine Optimal k for kNN

The built-in caret::train() function will run the KNN classification multiple times with different values for `k`, evaluate model performance, and experimentally determine the optimal value of `k` for our data.  

```{r}
# kNN with caret - autofit k
knn_caret <- train(train_penguins, 
                   species_actual_train, 
                   method = "knn", 
                   preProcess = c("center", "scale"))

knn_caret
```

From our output, we can see that a $k=9$ was found to be the optimal value for our model based on the calculated `Accuracy` and `Kappa` values. The inter-rater reliability metric, `Kappa`, is quite important when working with unbalanced data sets. Unbalanced datasets have different counts for each class.  As the count discrepancy between classes increases, our model can become biased by the larger represented class, leading to poorer predictions of the smaller classes.  Given this, a $k=9$ may ultimately perform better than our initial KNN model with $k=16$, especially on new data.  

```{r}
# plot accuracy vs # neighbors
plot(knn_caret)
```

We can see that a neighbors value of 9 clearly showed the highest accuracy value.  

```{r}
# apply model to holdout group
knn_caret_predictions <- predict(knn_caret, newdata = test_penguins)

# show confusion matrix and model performance KPIs
confusionMatrix(knn_caret_predictions, as.factor(species_actual_test))
```

In the end, with the `caret` package identifying the "optimal" k-value, we can see that both our KNN models ($k=16$ and $k=9$) performed very well on our test data set. Again, `k` indicates how many nearby data points are used to assign a sample data point to a class.  If our neighboring data points are spread out versus tightly clustered will influence the optimal `k`.  Large `k` values may lead to more misclassifications near borders between classes.  If classes overlap or have non-linear patterns, then smaller `k` values may better identify classes for pockets of data points.  
 
In this case, it appears that either of our models could be chosen to provide accurate predictions of the three penguin species.
 
***

## **Part 2: Decision Trees on loan approval data set**

***

**Please use the attached dataset on loan approval status to predict loan approval using Decision Trees. Please be sure to conduct a thorough exploratory analysis to start the task and walk us through your reasoning behind all the steps you are taking.**  

### Load Data

First, we decided to read in the loan approval data set and take a look at its features:

```{r}
# load CSV data set
loan <- read.csv("https://raw.githubusercontent.com/DaisyCai2019/NewData/master/Loan_approval.csv")

# display first few rows for inspection
kable(head(loan)) %>% 
  kable_styling(bootstrap_options = "basic")
```

As we can see from a glimpse of the data set above, the following features are available:  

+ **Loan_ID**: a unique identifier for each loan   
+ **Gender**: split into male/female  
+ **Married**: indicates whether the applicant is either married ("Yes") or not married ("No")  
+ **Dependents**: records the number of dependents to the applicant  
+ **Education**: indicates whether the applicant is a graduate or undergraduate student  
+ **Self_Employed**: indicates whether the applicant is either self employed ("Yes) or not ("No")  
+ **ApplicantIncome**: indicates the applicant's income  
+ **CoapplicantIncome**: indicates the coapplicant's income  
+ **LoanAmount**: indicates the loan amount (in thousands)  
+ **Loan_Amount_Term**: indicates the loan amount term in number of months  
+ **Credit_History**: indicates whether or not the applicant's credit history meets loan guidelines (1 or 0)  
+ **Property_Area**: indicates whether the applicant's property is "urban", "semi urban" or "rural"  
+ **Loan_Status**: the target variable, indicates whether or not the applicant received the loan   

***

### Exploratory data analysis  

Now, we can run some exploratory data analysis to get a better sense of how to tidy and interpret these features. Here's an initial summary of the dataset:  

```{r}
skim(loan)
```

#### Handle Missing Data

We can see that there are a fair amount of things we'll need to do to clean the dataset before being able to run our decision tree algorithm. First, we can see that there quite a few missing values (NAs) in our `LoanAmount`, `Loan_Amount_Term` and `Credit_History` features. Also, we noticed that there were a lot of blank values, which needed to be recoded to NAs. Therefore, we used the `naniar` package below to handle this:  

```{r}
loan <- loan %>% 
  replace_with_na_all(condition = ~. == "")
```


```{r}
loan %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  pivot_longer(cols = 1:13, names_to = 'columns', values_to = 'NA_count') %>%
  arrange(desc(NA_count)) %>%
  ggplot(aes(y = columns, x = NA_count)) + geom_col(fill = 'deepskyblue4') +
  geom_label(aes(label = NA_count)) +
  theme_minimal() +
  labs(title = 'Count of missing values in penguins dataset') +
  theme(plot.title = element_text(hjust = 0.5))
```
```{r}
gg_miss_upset(loan)
```

This can be further visualized by the "Missingness Map" below:  

```{r, message=FALSE, warning=FALSE}
missmap(loan)
```

#### Fix Datatypes

Before progressing, we thought it would be helpful to conduct some transformations on this data, as well as account for NAs. To do this, we used the `factor` function on each feature:  

```{r}
loan$Loan_Status <- factor(loan$Loan_Status)

loan <- loan %>%
           mutate(Gender = factor(Gender),
                  Married = factor(Married),
                  Dependents = factor(Dependents),
                  Education = factor(Education),
                  Self_Employed = factor(Self_Employed),
                  Property_Area = factor(Property_Area),
                  Loan_Status = factor(Loan_Status))

summary(loan)
```

### Feature Plots

Next, we'll want to take a look at the distributions of the continuous features, which appear to be `ApplicantIncome`, `CoapplicantIncome`, `LoanAmount`, and `Loan_Amount_Term`.  We color coded `Loan_status` where blue is `Y` and red is `N` to help visualize patterns between the distributions and our feature distribution:  

```{r echo=FALSE, fig.height=8, fig.width=8, message=FALSE, warning=FALSE}
loan %>%
  dplyr::select(ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Loan_Status) %>%
  ggpairs(aes(color = Loan_Status, alpha = 0.9))
```

We can see that `ApplicantIncome`, `CoapplicantIncome` and `LoanAmount` are highly right skewed, with long right tails. Conversely, it looks like `Loan_Amount_Term` is highly left skewed, with a long left tail.  

### Target Plots

Additionally, we can also take a look at the categorical variables, broken out by our target variable of whether or not a loan was issued to the applicant. From the green bars below, we can see the total amount of loan approvals, relative to the red bars below documenting the loan denials. 

```{r, echo=FALSE}
par(mfrow=c(2,3))

tbl1 <- table(loan$Loan_Status, loan$Gender)
barplot(tbl1, main="Loan Status by Gender",
        xlab="Gender", col=c("darksalmon","darkolivegreen3"))

tbl2 <- table(loan$Loan_Status, loan$Education)
barplot(tbl2, main="Loan Status by Education",
        xlab="Education", col=c("darksalmon","darkolivegreen3"))

tbl3 <- table(loan$Loan_Status, loan$Married)
barplot(tbl3, main="Loan Status by Married",
        xlab="Married", col=c("darksalmon","darkolivegreen3"))

tbl4 <- table(loan$Loan_Status, loan$Self_Employed)
barplot(tbl4, main="Loan Status by Self Employed",
        xlab="Self_Employed", col=c("darksalmon","darkolivegreen3"))

tbl5 <- table(loan$Loan_Status, loan$Property_Area)
barplot(tbl5, main="Loan Status by Property_Area",
        xlab="Property_Area", col=c("darksalmon","darkolivegreen3"))

tbl6 <- table(loan$Loan_Status, loan$Credit_History)
barplot(tbl6, main="Loan Status by Credit_History",
        xlab="Credit_History", col=c("darksalmon","darkolivegreen3"))
```

From the categorical variable splits based on loan approval status, we can see a few interesting things:  

+ The number of loans applied for by males was significantly higher than loans applied for by females in this dataset. 

+ The same phenomenon is true for graduate students, where a much higher number of loan applications were coming from graduate students relative to undergraduate students.  

+ Whether or not the applicant was self employed or married also showed a pretty large class imbalance, where those that identified as not self employed and those that identified as married had higher proportions of applicants in this dataset than those that did not identify by those two characteristics.  

+ There was a pretty even split in the number of applicants based on property area, where someone identifying that they live in *rural*, *semiurban*, or *urban* settings were pretty evenly distributed.  

+ We can see that applicants that did not pass the credit history criteria were almost always likely to be denied a loan.  

These factors will be interesting to examine and discuss as we look to build our decision tree model and random forest models, since these class imbalances could affect model performance and interpretation.  

***

### Impute missing

Since there were a large amount of missing values that we identified above, we can use KNN() to help with our imputations. With the KNN method, a categorical missing value is imputed with the majority among its k nearest neighbors, and the average value (mean) of the k nearest neighbors is regarded as the prediction for a numerical missing value.

```{r}
loan <- kNN(loan) %>%
  subset(select = Loan_ID:Loan_Status)
```

Now, we can double check that this worked correctly by running syntax to find any missing values:

```{r, warning=FALSE, message=FALSE}
sapply(loan, function(x) sum(is.na(x)))
```

Fortunately, we can see that there are now no missing data points!

### Apply Transformations

Next, we'll have to work through a few transformations for our highly skewed continuous data. For our `LoanAmount` feature, we can conduct a log transformation:  

```{r}
loan$LogLoanAmount <- log(loan$LoanAmount)
loan$LogLoan_Amount_Term <- log(loan$Loan_Amount_Term)
```

```{r, echo=FALSE}
par(mfrow=c(1,2))

hist(loan$LoanAmount, 
     main="Histogram for Loan Amount", 
     xlab="Loan Amount", 
     border="black", 
     col="gray",
     las=1, 
     breaks=20, prob = TRUE)

hist(loan$LogLoanAmount, 
     main="Histogram for Log Loan Amount", 
     xlab="Loan Amount (transformed)", 
     border="black", 
     col="gray",
     las=1, 
     breaks=20, prob = TRUE)
```

Banks generally don't want loan payments to exceed 25% of an applicants income.  Most banks will add the co-applicant income to the applicant income and treat the sum in their decision criteria.  For this reason, we combine `ApplicantIncome` and `CoapplicantIncome` into a single `Income` feature and drop the separate `ApplicantIncome` and `CoapplicantIncome`.  Next we perform a log transformation on `Income` to obtain the best results.  

```{r}
loan$Income <- loan$ApplicantIncome + loan$CoapplicantIncome
loan$ApplicantIncome <- NULL
loan$CoapplicantIncome <- NULL
loan$LogIncome <- log(loan$Income)
```

When checking the new distribution of our `Income` variable, we can see that it is much more normally distributed:  

```{r, echo=FALSE}
hist(loan$LogIncome, 
     main="Histogram for Applicant Income", 
     xlab="Income", 
     border="black", 
     col="gray",
     las=1, 
     breaks=50, prob = TRUE)
```

With the exploratory data analysis behind us, and our data tidy'd, we are now ready to run our decision tree!  

### Splitting our data into a Training/Test set

Similar to our KNN model process above, we'll determine an 80/20 split for our testing and training datasets:  

```{r}
set.seed(123)

trainingRows <- createDataPartition(as.matrix(species_actual$species), p=0.8, list=FALSE)

train_loan<-loan[trainingRows,]
test_loan<-loan[-trainingRows,]
```

### Initial Model

With the data split, we can now run our first decision tree:  

```{r}
tree1 <- rpart(Loan_Status ~ Gender + Married + Dependents + Education + 
                 Self_Employed + LogIncome + LogLoanAmount + Loan_Amount_Term + 
                 Credit_History + Property_Area, 
               data=train_loan)

rpart.plot(tree1, nn=TRUE)
summary(tree1)
```

### Decision Tree Predictions

From our decision tree summary and plot, we can see that a few variables, such as `Credit_History`, `Income` and `LoanAmount`, were the most important features in this classification method. We can use this decision tree to make predictions on our holdout test set.

```{r}
loanPre <- predict(tree1, 
                   test_loan, 
                   type="class",
                   control = rpart.control(minsplit = 20, xval = 81, cp = 0.01))

confusionMatrix(loanPre, test_loan$Loan_Status)
```

As we can see from our predictions and the confusion matrix, we were able to obtain about 68% accuracy using this decision tree. To achieve higher accuracy, we'll use the `prune()` function in the `rpart` package to examine a predicted optimal tree size. 

```{r}
plotcp(tree1)
```
By plotting the cross-validated error against the complexity parameter, we can see that the relationship between the yields the best optimal outcome for our tree at a tree size of 2. 

```{r}
tree1$cptable
```

### Tuning our Decision Tree

Although this is the case, when using this cp value in our `prune()` function, we still obtain accuracy ratings of around 68%. In order to test different complexity parameters, we also decided to use our cp value of 0.01647059, which is a tree size of 2. When we use this tree size, and prune our initial tree accordingly, we can see the resulting decision tree below:  

```{r}
tree1.pruned <- prune(tree1, cp=0.01647059)

rpart.plot(tree1.pruned, 
           extra = 104, 
           main = "Decision Tree")
```

Interestingly, from this pruning process, the `Credit_History` feature alone seems to do a fairly good job of classifying applicants into the approval vs. disapproval status. The accuracy is about 86% on our test set, which is slightly higher than our initial tree.  

However, it's important to think critically about whether or not this pruned tree would perform better on another dataset, and if it is applicable for real-life events. 

```{r}
tree1.pruned.pred <- predict(tree1.pruned, 
                             test_loan, 
                             type = "class" )

confusionMatrix(tree1.pruned.pred, test_loan$Loan_Status)
```

While a single feature does offer fairly good accuracy, there may be mitigating information from other features that would affect a loan decision which is lost if we only include a single feature.  While decision trees are simple to construct and understand, their weakness is that they follow a simple linear path for decision making.  This linear flow doesn't allow for downstream features to alter or influence potential predictions.

***

## **Part 3: Random Forests on loan approval dataset**

***

**Using the same dataset on Loan Approval Status, please use Random Forests to predict on loan approval status. Again, please be sure to walk us through the steps you took to get to your final model. (50 points)**

To expand on our analysis of the loan approval dataset, we'll run a few random forest models to see if they are a more effective way to classify whether or not someone will be accepted or rejected for a loan.

### Our Initial Random Forest Model

In order to run a random forest model, we'll use the `randomForest()` function in the `randomForest` package. We'll include the same initial features that we did for our first decision tree:  

```{r}
set.seed(123)
fit.forest <- randomForest(Loan_Status ~ Gender + Married + Dependents + 
                             Education + Self_Employed + LogIncome + 
                             LogLoanAmount + Loan_Amount_Term + 
                             Credit_History + Property_Area,
                           data=train_loan)

fit.forest

importance(fit.forest)
```
After fitting our model, we can now use it to create predictions on our holdout test set to evaluate its performance. 

```{r}
forest.pred <- predict(fit.forest, 
                       newdata = test_loan)

(dtree.cm_train <- confusionMatrix(forest.pred, test_loan$Loan_Status))
```

Here, we notice a slight improvement on both samples where accuracy for the training sample is 82.37%. Notice the disparity between Specificity and Sensitivity.  This model has a high false positive rate and tends to over predict `Y` when it should have chosen `N`.

Next, we want to see if we have generated enough trees so that the Out Of Bag (OOB Error) error rates are minimum. From the below we see that the  OOB error rate is decreasing with 1-20 trees, and rate stabilizes that at around 100 trees.

```{r}
plot(fit.forest, 
     col = c("black", "red", "dark green"), 
     main = "Predicted Loan Error Rates")

legend("topright", colnames(fit.forest$err.rate), col = 1:6, cex = 0.8, fill = 1:6)
```

### Tuning our Random Forest Model

In order to see whether the Random forest model can be improved, we will run the same model, but this time we will use the `tuneRF` function. We provide the features, x, target, y, and number of trees used at each tuning step, `ntreeTry`.

The values were assigned randomly initially, and they are then tweaked until the optimal value was found.

```{r}
set.seed(123)

tree_var <- tuneRF(x = subset(train_loan, select = -Loan_Status), 
                   y = train_loan$Loan_Status, 
                   ntreeTry = 1000)
```

We rerun the Random Forest model with the new parameter $Ntree = 1000$ and $Mtry = 3$

```{r}
set.seed(123)

val_opt <- tree_var [,"mtry"][which.min(tree_var [,"OOBError"])]

fit.forest2 <- randomForest(Loan_Status  ~ Gender + Married + Dependents + 
                              Education + Self_Employed + LogIncome + 
                              LogLoanAmount + Loan_Amount_Term + 
                              Credit_History + Property_Area,
                            data=train_loan, 
                            importance = TRUE, 
                            ntree=1000, 
                            mtry = val_opt)

fit.forest2
```

The results of the Random Forest model on our training set yielded an out of bag error rate of 22.39%, which is lower than the original model's OOB error of 23.88%. Therefore, we can see that tuning our model did help to reduce the OOB error slightly.  

```{r}
forest.pred2 <- predict(fit.forest2, 
                        newdata = test_loan)

(forest.cm_train <- confusionMatrix(forest.pred2, test_loan$Loan_Status))
```

When we subject our second Random Forest Model to make predictions on our test set, we notice that there is slightly higher accuracy than our previous model, which is roughly 84.1%. The tuned model has performed better than the test data set than the original Random Forest. The accuracy is slightly increased, and the 95 % CI has increase a bit too. 

Next we want to see if we have generated enough trees so that the Out Of Bag (OOB Error) error rates are minimum. From the below we see that the  OOB error rate is decreasing with 1-20 trees, increasing a little bit with 20-50 trees, and rate stabilizes that at around 100 trees.

```{r}
plot(fit.forest2, 
     col = c("black", "red", "dark green"), 
     main = "Predicted Loan Error Rates")

legend("topright", colnames(fit.forest2$err.rate), col = 1:6, cex = 0.8, fill = 1:6)
```

***

## **Part 4: Gradient Boosting**

***

**Using the Loan Approval Status data, please use Gradient Boosting to predict on the loan approval status. Please use whatever boosting approach you deem appropriate;but please be sure to walk us through your steps. (50 points)**

### Initial Model

```{r}
set.seed(123)

boost <- gbm(Loan_Status~., 
             data=train_loan[,-1],
             distribution = "gaussian",
             n.trees = 1000,
             cv.folds = 3)
```

gbm uses a default number of trees of 100, which is rarely sufficient. Consequently, I crank it up to 1,000 trees. The default depth of each tree (interaction.depth) is 1, which means we are ensembling a bunch of stumps. Lastly, I also include cv.folds to perform a 3 fold cross validation. 

```{r}
boost
sqrt(min(boost$cv.error))
summary(boost)
```
The best cross-validation iteration was 34 and the RMSE is 0.3913894

The summary of output creates a new data set with var, a factor variable with the variables in our model, and rel.inf, the relative influence each variable had on our model predictions.From the table, we can see `LoanAmount`, `LogIncome` and `Credit_History` are the top three most important variable for our model.

### Tuning

We could tune parameters to see how the results change. Here, I increase the number of trees and also perform a 5 fold cross validation.

```{r}
set.seed(123)

boost2<-gbm(Loan_Status~., 
            data=train_loan[,-1],
            distribution = "gaussian",
            n.trees = 2000,
            cv.folds = 5)

sqrt(min(boost2$cv.error))

summary(boost2)
```

We can see the number of relative influence of each variable change and also the RMSE change to 0.3900092, lower than our previous model.

The partial Dependence Plots tell us the relationship and dependence of the variables X with the Response variable Y.

```{r}
#partial dependence plots
plot(boost2,i="LoanAmount")
plot(boost2,i="Income")

```
The first plot shows loanAmount is negatively correlated with the response Loan_Status before 300K. The second plot indicate income is positively correlated with Loan_Status when it less than $20,000.

```{r warning=FALSE}
# plot loss function as a result of n trees added to the ensemble
gbm.perf(boost2, method = "cv")
gbm.perf(boost2, method = "OOB")

```

The plots indicating the optimum number of trees based on the technique we used. The green line indicates error on test and the blue dotted line points the optimum number of iterations. We can observe that the beyond a certain a point (55 iterations for the `cv` method and 40 for the `OOB` method), the error on the test data appears to increase because of overfitting. 

We use model 2 to forecast our data. According to the `cv` method, we choose 55 as the number of trees.

```{r}
boostPre <- predict.gbm(boost2, 
                        test_loan, 
                        n.trees = 55)

boostPre <- ifelse(boostPre < 1.5, 'N', 'Y')
boostPre <- as.factor(boostPre)

(boost.cm <- confusionMatrix(boostPre, test_loan$Loan_Status))
```

According to the Confusion matrix, Our model accuracy is 0.841. Looking at Sensitivity and Specificaity, this model is has a far lower False Negative rate and is less likely to pick 'N' when it shouldn't.  However, it's False positive rate is higher and where it picked 'Y' when it should have predicted 'N'. 

***

## **Part 5: Model Performance**

***

**Model performance: please compare the models you settled on for problem # 2 – 4. Comment on their relative performance. Which one would you prefer the most? Why?(20 points)**

After running various decision trees, random forest models, and gradient boosting methods, we can take a look at the overall evaluation metrics for these three techniques on the loan approval dataset. By creating a dataframe to store all of our metrics, we can visualize the outcomes below:    

```{r, echo=FALSE}
temp <- data.frame(dtree.cm_train$overall, 
                   forest.cm_train$overall,
                   boost.cm$overall) %>%
  t() %>%
  data.frame() %>%
  dplyr::select(Accuracy) %>%
  mutate(`Classification Error Rate` = 1-Accuracy)
```

```{r, echo=FALSE}
eval <- data.frame(dtree.cm_train$byClass, 
                   forest.cm_train$byClass,
                   boost.cm$byClass)
eval <- data.frame(t(eval)) %>%
  cbind(temp) %>%
  mutate(eval = c("Decision Tree", "Random Forest", "Gradient Boosting")) 
```

```{r}
eval <- dplyr::select(eval, Accuracy, `Classification Error Rate`, Sensitivity, Specificity, Precision, Recall, F1)
rownames(eval) = c("Decision Tree",  "Random Forest", "Gradient Boosting")
t_eval <- t(eval)
colnames(t_eval) <- rownames(eval)
rownames(t_eval) <- colnames(eval)
knitr::kable(t_eval)
```

When comparing our three different techniques to one another, we can see that overall, our decision tree method seemed to perform the worst. With lower overall accuracy, higher classification error rate, and worse performance on measures such as F1, sensitivity and specificity, we can determine that we'd likely not prefer the decision tree approach. Additionally, the decision tree we are using here to evaluate against our other techniques only utilizes one feature (`Credit_History`), and such isn't as flexible if we were to utilize this model on a different dataset.  

When considering model performance, `accuracy` is a good starting point, but from a business perspective, false positives versus false negatives can be quite important.  If the bank incorrectly predicts they shouldn't give a loan, but the applicant would have paid it (false negative), the bank looses out on potential revenue.  If the bank incorrectly gives a loan and the applicant defaults (false positive), then the bank loses money from defaults.  In the end, the bank will need to assign a cost function to both of these situations and determine which has a more negative impact on business.  With all three models, we see high `specificity` between 94~99% which indicates low false negative.  These models do a pretty good job of giving loans to applicants that will repay so there should be little opportunity cost on that side.  However, we see low sensitivity, i.e., high numbers of false positives.  These models would suggest giving loans to applicants that default.  Of the two errors, this second is probably far more expensive.  As a bank we would probably care more about maximizing `Sensitivity` along with `Accuracy`.  With that in mind, the Decision Tree might end up being a better compromise from a business perspectiuve.

When looking at the overall performance of our Random Forest model and Gradient Boosting approach on our holdout test set, we can see...










