---
title: "Data 622 - Homework #3 "
author: "Mengqin Cai, Zhi Ying Chen, Donny Lofland, Grace Han, Zach Alexander"
date: "4/5/2021"
output: 
  pdf_document:
        extra_dependencies: ["geometry", "multicol", "multirow"]
  md_document:
    variant: markdown_github
  html_document:
    df_print: paged
    toc: yes
    toc_collapsed: yes
    toc_float: yes
theme: lumen
number_sections: yes
toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,include=FALSE}
require(palmerpenguins)
require(dplyr)
require(ggplot2)
require(caret)
require(pROC)
require(GGally)
require(tidyr)
require(Amelia)
require(VIM)
require(MASS)
require(psych)
require(class)
require(tree)
require(rpart)
require(rpart.plot)
require(randomForest)
require(knitr)
require(kableExtra)
require(naniar)
require(mice)
require(gbm)
require(xgboost)
```


***

#### **Part 1: KNN on the Penguins dataset**


***

**Please use K-nearest neighbor (KNN) algorithm to predict the species variable. Please be sure to walk through the steps you took. (40 points)**

Similar to past assignments when using the Palmer Penguins dataset, we'll first do some quick exploratory analysis to examine the different features available.  

We can see below, that there are four continuous variables: `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm` and `body_mass_g`. Additionally, there are a few categorical variables: `island`, `sex`, and `year`.  

```{r, echo=FALSE}
penguins <- data.frame(penguins)

kable(head(penguins)) %>% kable_styling(bootstrap_options = "basic")
```

For the continuous variables, we can examine the distributions, broken out by the target variable, `species`:  

```{r, message=FALSE, warning=FALSE, fig.width=12, fig.height=10, echo=FALSE}
penguins %>%
  dplyr::select(species, body_mass_g, bill_length_mm, bill_depth_mm, flipper_length_mm) %>%
  ggpairs(aes(color = species))
```

By separating our distributions by our target variable, `species`, we can see that many of the feature interactions show clustering between Adelie and Chinstrap penguins (red and green), while Gentoo penguins tend to contrast the other two species for most interactions. This is also confirmed by most of the single-variable distributions split out by species in the plots below. With the exception of the distribution of `bill_length_mm`, distributions for `body_mass_g`, `bill_depth_mm`, and `flipper_length_mm` all show there to be overlapping distributions between Adelie and Chinstrap penguin species.  

Next, we'll do some data tidying to get our dataset ready for our KNN model. Since `year` reflects the date/time of recording, and is not beneficial for our machine learning algorithm, we'll remove it from our dataset:  

```{r}
penguins <- penguins %>% dplyr::select(-year)
```

Additionally, when looking at the number of missing values, we can see the following:  

```{r}
penguins %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  pivot_longer(cols = 1:7, names_to = 'columns', values_to = 'NA_count') %>%
  arrange(desc(NA_count)) %>%
  ggplot(aes(y = columns, x = NA_count)) + geom_col(fill = 'deepskyblue4') +
  geom_label(aes(label = NA_count)) +
  theme_minimal() +
  labs(title = 'Count of missing values in penguins dataset') +
  theme(plot.title = element_text(hjust = 0.5))
```
We can see that 11 individuals have at least one missing data point. Therefore, we'll drop these from our dataset as well.  

```{r}
penguins <- na.omit(penguins)
```

Now, we can take a look at a summary of data before splitting to get a sense of what still needs to be tidy'd in order to get it ready for our KNN model:  

```{r}
summary(penguins)
```
From above, we can see that we'll need to remove our `species` variable, and save it to a separate outcome variable in order to evaluate the performance of our KNN model later on. We can do this by running the following syntax:  

```{r}
species_actual <- penguins %>% dplyr::select(species)
penguins <- penguins %>% dplyr::select(-species)
```

Additionally, we can see below in the distributions of each of our continuous variables that the scales are inconsistent across features. For better model performance, we'll want to standardize each of our features.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
penguins %>%
  dplyr::select(body_mass_g, bill_length_mm, bill_depth_mm, flipper_length_mm) %>%
  ggpairs()
```
In order to do this, we'll administer a z-score standardization to fix our scaling, which will ultimately help with our clustering.

```{r}
penguins[, c("bill_length_mm","bill_depth_mm", "flipper_length_mm", "body_mass_g")] <- scale(penguins[, c("bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g")])
```

As you can see below, after the z-score standardization, the scaling on the x and y axis is a lot more consistent across our features.
```{r, message=FALSE, warning=FALSE}
penguins %>%
  dplyr::select(body_mass_g, bill_length_mm, bill_depth_mm, flipper_length_mm) %>%
  ggpairs()
```
Although this may help with our KNN model, we'll have to be careful about interpretability later on! Finally, we'll want to dummy code our `island` and `sex` variables, since they are categorical, we'll need to convert them to 1s and 0s:  

```{r}
island_dcode <- as.data.frame(dummy.code(penguins$island))
penguins <- cbind(penguins, island_dcode)
```

We'll also want to remove our original `island` variable from this dataset, since we created our dummy variables:  

```{r}
penguins <- penguins %>% dplyr::select(-island)
```

The same process will then be applied to the `sex` variable:  

```{r}
penguins$sex <- dummy.code(penguins$sex)
```

Finally, after a fair amount of data tidying and investigation, our dataset is ready for our KNN model. We can take a quick look at the updated version of our dataset:  

```{r, warning=FALSE, message=FALSE}
kable(head(penguins)) %>% kable_styling(bootstrap_options = "basic")
```
***

***Splitting into training and testing datasets***  


With our dataset tidy'd, we were then able to split it into a training and test set.

```{r}
sample_size<-floor(0.8*nrow(penguins))

set.seed(123)
train_ind<-sample(seq_len(nrow(penguins)),size=sample_size)
train_penguins<-penguins[train_ind,]
test_penguins<-penguins[-train_ind,]
```

We'll also do the same for our `target` variable, using the same split:  

```{r}
species_actual_train <- species_actual$species[train_ind]
species_actual_test <- species_actual$species[-train_ind]
```

***

***Fitting the KNN model***  

Now, with our data split accordingly, we'll need to identify the appropriate number for k. To do this, we will first take a standard approach of calculating the square root of the number of rows in our training dataset:  

```{r}
sqrt(nrow(train_penguins))
```
We can see above, that it is roughly between 16 and 17. Therefore, we'll perform two KNN models with k=16 and k=17:

```{r}
set.seed(123) 
k16<-knn(train_penguins,test_penguins,cl=species_actual_train,k=16)
k17<-knn(train_penguins,test_penguins,cl=species_actual_train,k=17)
```
```{r}
misClassError <- mean(k16 != species_actual_test)
paste0('The number of misclassified penguins with 16 neighbors is: ', misClassError)
table(k16, species_actual_test)


misClassError <- mean(k17 != species_actual_test)
paste0('The number of misclassified penguins with 17 neighbors is: ', misClassError)
table(k17, species_actual_test)
```
From our resulting confusion matrices, that show the classified penguins from our KNN model compared to the actual species designations, we can see that both a k of 16 and a k of 17 preformed well on our test set. Essentially, the misclassification rate showed no difference with k=16 or k=17. Although this approach seemed to work quite effectively, we do need to be careful about how we'd go about using either one of these models to predict new data. With an artificially high k value, we could be underfitting the data.  

Therefore, as a final step, we can run one final KNN model using the `caret` package, where the built-in `train()` function will run KNN classification that automatically picks the optimal number of neighbors (k).  

```{r}
knn_caret <- train(train_penguins, species_actual_train, method = "knn", preProcess = c("center", "scale"))

knn_caret
```
From our output, we can see that a k value of 9 was chose as the optimal value for our model based on the Accuracy and Kappa values calculated. Since this inter-rater reliability metric of Kappa is quite important when working with unbalanced datasets such as our penguins dataset, this k value of 9 may ultimately perform better than our initial KNN models where k=16 or k=17. We can see this relationship determined by the `train` function for finding the optimal neighbors plotted below:  

```{r}
plot(knn_caret)
```

We can see that a neighbors value of 9 clearly showed the highest accuracy value.  

```{r}
knn_caret_predictions <- predict(knn_caret, newdata = test_penguins)
confusionMatrix(knn_caret_predictions, as.factor(species_actual_test))
```
In the end, with the `caret` package identifying the "optimal" k-value, we can see that all three of our KNN models performed very well on our test dataset. Although it will likely depend on the utility of the KNN model, it does appear that any of our three models could be chose to provide accurate predictions of the three penguin species.  

***

#### **Part 2: Decision Trees on loan approval dataset**


***

**Please use the attached dataset on loan approval status to predict loan approval using Decision Trees. Please be sure to conduct a thorough exploratory analysis to start thetask and walk us through your reasoning behind all the steps you are taking.**  

First, we decided to read in the loan approval dataset and take a look at its features:

```{r}
loan <- read.csv("https://raw.githubusercontent.com/DaisyCai2019/NewData/master/Loan_approval.csv")
kable(head(loan)) %>% kable_styling(bootstrap_options = "basic")
```

As we can see from a glimpse of the dataset above, the following features are available:  

+ **Loan_ID**: a unique identifier for each loan   
+ **Gender**: split into male/female  
+ **Married**: indicates whether the applicant is either married ("Yes") or not married ("No")  
+ **Dependents**: records the number of dependents to the applicant  
+ **Education**: indicates whether the applicant is a graduate or undergraduate student  
+ **Self_Employed**: indicates whether the applicant is either self employed ("Yes) or not ("No")  
+ **ApplicantIncome**: indicates the applicant's income  
+ **CoapplicantIncome**: indicates the coapplicant's income  
+ **LoanAmount**: indicates the loan amount (in thousands)
+ **Loan_Amount_Term**: indicates the loan amount term in number of months  
+ **Credit_History**: indicates whether or not the applicant's credit history meets loan guidelines (1 or 0)  
+ **Property_Area**: indicates whether the applicant's property is "urban", "semi urban" or "rural"  
+ **Loan_Status**: the target variable, indicates whether or not the applicant received the loan   

***

***Exploratory data analysis of the loan approval dataset***  

Now, we can run some exploratory data analysis to get a better sense of how to tidy and interpret these features. Here's an initial summary of the dataset:  
```{r}
summary(loan)
```
We can see that there are a fair amount of things we'll need to do to clean the dataset before being able to run our decision tree algorithm. First, we can see that there quite a few missing values (NAs) in our `LoanAmount`, `Loan_Amount_Term` and `Credit_History` features. Also, we noticed that there were a lot of blank values, which needed to be recoded to NAs. Therefore, we used the `naniar` package below to handle this:  

```{r}
loan <- loan %>% replace_with_na_all(condition = ~. == "")
```


```{r}
loan %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  pivot_longer(cols = 1:13, names_to = 'columns', values_to = 'NA_count') %>%
  arrange(desc(NA_count)) %>%
  ggplot(aes(y = columns, x = NA_count)) + geom_col(fill = 'deepskyblue4') +
  geom_label(aes(label = NA_count)) +
  theme_minimal() +
  labs(title = 'Count of missing values in penguins dataset') +
  theme(plot.title = element_text(hjust = 0.5))
```


This can be further visualized by the "Missingness Map" below:  

```{r, message=FALSE, warning=FALSE}
missmap(loan)

```



Before progressing, we thought it would be helpful to conduct some transformations on this data, as well as account for our N/As that we've discovered. To do this, we used the `factor` function on each feature:  

```{r}
loan$Loan_Status<-factor(loan$Loan_Status)


loan<-loan %>%
           mutate(Gender = factor(Gender),
                  Married = factor(Married),
                  Dependents=factor(Dependents),
                  Education=factor(Education),
                  Self_Employed=factor(Self_Employed),
                  Property_Area=factor(Property_Area),
                  Loan_Status=factor(Loan_Status))

summary(loan)
```

Next, we'll want to take a look at the distributions of the continuous features, which appear to be `ApplicantIncome`, `CoapplicantIncome`, `LoanAmount`, and `Loan_Amount_Term`:  

```{r, message=FALSE, warning=FALSE, echo=FALSE}
loan %>%
  dplyr::select(ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term) %>%
  ggpairs()
```

We can see that `ApplicantIncome`, `CoapplicantIncome` and `LoanAmount` are highly right skewed, with long right tails. Conversely, it looks like `Loan_Amount_Term` is highly left skewed, with a long left tail.  

Additionally, we can also take a look at the categorical variables, broken out by our target variable of whether or not a loan was issued to the applicant. From the green bars below, we can see the total amount of loan approvals, relative to the red bars below documenting the loan denials. 

```{r, echo=FALSE}
par(mfrow=c(2,3))
tbl1 <- table(loan$Loan_Status, loan$Gender)
barplot(tbl1, main="Loan Status by Gender",
        xlab="Gender", col=c("darksalmon","darkolivegreen3"))
tbl2 <- table(loan$Loan_Status, loan$Education)
barplot(tbl2, main="Loan Status by Education",
        xlab="Education", col=c("darksalmon","darkolivegreen3"))
tbl3 <- table(loan$Loan_Status, loan$Married)
barplot(tbl3, main="Loan Status by Married",
        xlab="Married", col=c("darksalmon","darkolivegreen3"))
tbl4 <- table(loan$Loan_Status, loan$Self_Employed)
barplot(tbl4, main="Loan Status by Self Employed",
        xlab="Self_Employed", col=c("darksalmon","darkolivegreen3"))
tbl5 <- table(loan$Loan_Status, loan$Property_Area)
barplot(tbl5, main="Loan Status by Property_Area",
        xlab="Property_Area", col=c("darksalmon","darkolivegreen3"))
tbl6 <- table(loan$Loan_Status, loan$Credit_History)
barplot(tbl6, main="Loan Status by Credit_History",
        xlab="Credit_History", col=c("darksalmon","darkolivegreen3"))
```

From the categorical variable splits based on loan approval status, we can see a few interesting things:  

+ The number of loans applied for by males was significantly higher than loans applied for by females in this dataset. 

+ The same phenomenon is true for graduate students, where a much higher number of loan applications were coming from graduate students relative to undergraduate students.  

+ Whether or not the applicant was self employed or married also showed a pretty large class imbalance, where those that identified as not self employed and those that identified as married had higher proportions of applicants in this dataset than those that did not identify by those two characteristics.  

+ There was a pretty even split in the number of applicants based on property area, where someone identifying that they live in rural, semiurban, or urban settings were pretty evenly distributed.  

+ We can see that applicants that did not pass the credit history criteria were almost always likely to be denied a loan.  

These factors will be interesting to examine and discuss as we look to build our decision tree model and random forest models, since these class imbalances could affect model performance and interpretation.  

***

***Imputing our data to reconcile missing values (NAs) and adjust transformations***  

Since there were a large amount of missing values that we identified above, we can use KNN() to help with our imputations. With the kNN method, a categorical missing value is imputed with the majority among its k nearest neighbors, and the average value (mean) of the k nearest neighbors is regarded as the prediction for a numerical missing value.

```{r}
loan<-kNN(loan)%>%
          subset(select = Loan_ID:Loan_Status)
```


Now, we can double check that this worked correctly by running syntax to find any missing values:
```{r, warning=FALSE, message=FALSE}
sapply(loan, function(x) sum(is.na(x)))
```
Fortunately, we can see that there are now no missing data points!

Next, we'll have to work through a few transformations for our highly skewed continuous data. For our `LoanAmount` feature, we can conduct a log transformation:  

```{r}
loan$LogLoanAmount <- log(loan$LoanAmount)
loan$LogLoan_Amount_Term <- log(loan$Loan_Amount_Term)
```

```{r, echo=FALSE}
par(mfrow=c(1,2))
hist(loan$LoanAmount, 
     main="Histogram for Loan Amount", 
     xlab="Loan Amount", 
     border="black", 
     col="gray",
     las=1, 
     breaks=20, prob = TRUE)
hist(loan$LogLoanAmount, 
     main="Histogram for Log Loan Amount", 
     xlab="Loan Amount (transformed)", 
     border="black", 
     col="gray",
     las=1, 
     breaks=20, prob = TRUE)
```

Next, since there is a lot of relateability between the `ApplicantIncome` variable and `CoapplicantIncome` variable, we thought it would be best to combine these two variables to create an `Income` variable, and then perform a log transformation to obtain the best results.  

```{r}
loan$Income <- loan$ApplicantIncome + loan$CoapplicantIncome
loan$ApplicantIncome <- NULL
loan$CoapplicantIncome <- NULL
loan$LogIncome <- log(loan$Income)
```

When checking the new distribution of our `Income` variable, we can see that it is much more normally distributed:  

```{r, echo=FALSE}
hist(loan$LogIncome, 
     main="Histogram for Applicant Income", 
     xlab="Income", 
     border="black", 
     col="gray",
     las=1, 
     breaks=50, prob = TRUE)
```
With the exploratory data analysis behind us, and our data tidy'd, we are now ready to run our decision tree!  

***

***Splitting loan dataset into training and testing datasets***  

Similar to our KNN model process above, we'll determine an 80/20 split for our testing and training datasets:  

```{r}
set.seed(123)
train_sample<-sample(1:nrow(loan),size = floor(0.80*nrow(loan)))
train_loan<-loan[train_sample,]
test_loan<-loan[-train_sample,]
```

***

***Splitting loan dataset into training and testing datasets***  

With the data split, we can now run our first decision tree:  

```{r}
tree1 <- rpart(Loan_Status~Gender+Married+Dependents+Education+Self_Employed+LogIncome+LogLoanAmount+Loan_Amount_Term+Credit_History+Property_Area,data=train_loan)
rpart.plot(tree1,nn=TRUE)
summary(tree1)
```

From our decision tree summary and plot, we can see that a few variables, such as `Credit_History`, `Income` and `LoanAmount`, were important features in this classification method. We can use this decision tree to make predictions on our holdout test set.

```{r}
loanPre<-predict(tree1,test_loan,type="class")
table(loanPre,test_loan$Loan_Status,dnn=c("Actual", "Predicted"))
accuracy_tree1<-mean(loanPre==test_loan$Loan_Status)
accuracy_tree1
```
As we can see from our predictions and the ensuing confusion matrix, we were able to obtain about 79% accuracy using this decision tree. To achieve higher accuracy, we'll use the `prune()` function in the `rpart` package to examine a predicted optimal tree size. 


```{r}
dtree.cm_train <- confusionMatrix(loanPre, test_loan$Loan_Status)
dtree.cm_train
```


```{r}
plotcp(tree1)
```
By plotting the cross-validated error against the complexity parameter, we can see that the relationship between the yields the best optimal outcome for our tree at a tree size of 2. 

```{r}
tree1$cptable
```
Although this is the case, when using this cp value in our `prune()` function, we still obtain accuracy ratings of around 79%. In order to test difference complexity parameters, we also decided to use our cp value of 0.41025641, which is a tree size of 1. When we use this tree size, and prune our initial tree accordingly, we can see the resulting decision tree below:  

```{r}
tree1.pruned <- prune(tree1, cp=0.41025641)
prp(tree1.pruned, type = 2, extra = 104, fallen.leaves = TRUE, main = "Decision Tree")
```

Interestingly, from this pruning process, the `Credit_History` feature alone seems to do a fairly good job of classifying applicants into the approval vs. disapproval status. The accuracy is about 83% on our test set, which is slightly higher than our initial tree.  

However, it's important to think critically about whether or not this pruned tree would perform better on another dataset, and if it is applicable for real-life events. 
```{r}
tree1.pruned.pred <- predict(tree1.pruned, test_loan, type = "class" )
tree1.pruned.perf <- table(test_loan$Loan_Status, tree1.pruned.pred, dnn = c("Actual", "Predicted"))

tree1.pruned.perf
accuracy_tree2<-mean(tree1.pruned.pred==test_loan$Loan_Status)
accuracy_tree2
```

```{r}
dtree.cm_pruned <- confusionMatrix(tree1.pruned.pred, test_loan$Loan_Status)
dtree.cm_pruned
```



In the end, we'd likely want to go with a more robust decision tree, than just one feature, but not getting too large in size -- which is why our initial tree seems to be a good balance between these two phenomenon.  

***

#### **Part 3: Random Forests on loan approval dataset**


***

**Using the same dataset on Loan Approval Status, please use Random Forests to predict on loan approval status. Again, please be sure to walk us through the steps you took to get to your final model. (50 points)**


```{r}
fit.forest <- randomForest(Loan_Status~Gender+Married+Dependents+Education+Self_Employed+LogIncome+LogLoanAmount+Loan_Amount_Term+Credit_History+Property_Area,data=train_loan)

fit.forest
importance(fit.forest)
```

```{r}
forest.pred <- predict(fit.forest, newdata = test_loan)
forest.cm <- table(test_loan$Loan_Status, forest.pred,
                     dnn=c("Actual", "Predicted"))
forest.cm
```

```{r}
accuracy_forest1 <- mean(forest.pred==test_loan$Loan_Status)
accuracy_forest1
```
Here, we notice slight improvements on both samples where accuracy for the training sample is 83.74%. 

```{r}
forest.cm_train <- confusionMatrix(forest.pred, test_loan$Loan_Status)
forest.cm_train
```


Next we want to see if we have generated enough trees so that the Out Of Bag (OOB Error) error rates are minimum. From the below we see that the  OOB error rate is decreasing with 1-20 trees, and rate stabilizes that at around 100 trees.
```{r}
plot(fit.forest, col = c("black", "red", "dark green"), main = "Predicted Loan Error Rates")
legend("topright", colnames(fit.forest$err.rate),col = 1:6, cex = 0.8, fill = 1:6)
```


In order to see whether the Random forest model can be improved, We will run the same model but this time we will use tuneRF function. X request for the variable of the dataset, but the target value, while Y is the target value. Stepfactor increases or decreases the Mtry at each iteration. The plot is whether to plot the OOB error as a function of Mtry. Ntreetry is the number of the number of trees used at the tuning step. Trace is whether to print the progress of the search and Improve the (relative) improvement in OOB error must be by this much for the search to continue.

The values were assigned randomly initially, and they have tweaked until the optimal was found.

```{r}
tree_var <- tuneRF(x = subset(train_loan, select = -Loan_Status), y=train_loan$Loan_Status, ntreeTry = 1000)
```

The Random Forest model is re-run with the new parameter Ntree= 1000 and Mtry=3

```{r}
val_opt <- tree_var [,"mtry"][which.min(tree_var [,"OOBError"])]
fit.forest2 <- randomForest(Loan_Status ~Gender+Married+Dependents+Education+Self_Employed+LogIncome+LogLoanAmount+Loan_Amount_Term+Credit_History+Property_Area,data=train_loan, importance = TRUE, ntree=1000, mtry = val_opt)

fit.forest2
```


The results of the trained Random Forest model are an out of bag error of 21.18 %, which is higher than the original model 19.35%, Although, it still a good result it has got worse with the tune. 

```{r}
forest.pred2 <- predict(fit.forest2, newdata = test_loan)
forest.cm2 <- table(test_loan$Loan_Status, forest.pred2,
                     dnn=c("Actual", "Predicted"))
forest.cm2
```

```{r}
accuracy_forest2 <- mean(forest.pred2==test_loan$Loan_Status)
accuracy_forest2
```

```{r}
forest.cm_tune <- confusionMatrix(forest.pred2, test_loan$Loan_Status)
forest.cm_tune
```


Here, we notice there has the same accuracy as previous one for the training sample is 82.11%. The tuned model has performed worse than the test data set than the original Random Forest. The accuracy is slightly decreased, and the 95 % CI has decreased a bit too. 


Next we want to see if we have generated enough trees so that the Out Of Bag (OOB Error) error rates are minimum. From the below we see that the  OOB error rate is decreasing with 1-20 trees, and rate stabilizes that at around 20 trees.
```{r}
plot(fit.forest2, col = c("black", "red", "dark green"), main = "Predicted Loan Error Rates")
legend("topright", colnames(fit.forest2$err.rate),col = 1:6, cex = 0.8, fill = 1:6)
```





***

#### **Part 4: Gradient Boosting**


***

**Using the Loan Approval Status data, please use Gradient Boosting to predict on the loan approval status. Please use whatever boosting approach you deem appropriate;but please be sure to walk us through your steps. (50 points)**

```{r}

set.seed(1)
boost<-gbm(Loan_Status~., data=train_loan[,-1],distribution = "gaussian",n.trees = 1000,cv.folds = 3)
```

gbm uses a default number of trees of 100, which is rarely sufficient. Consequently, I crank it up to 1,000 trees. The default depth of each tree (interaction.depth) is 1, which means we are ensembling a bunch of stumps. Lastly, I also include cv.folds to perform a 3 fold cross validation. 


```{r}
boost
sqrt(min(boost$cv.error))
summary(boost)
```
The best cross-validation iteration was 34 and the RMSE is 0.3913894
The summary of output creates a new data set with var, a factor variable with the variables in our model, and rel.inf, the relative influence each variable had on our model predictions.From the table, we can see LoanAmount, income and Credit_History is the top three most important vaiable for our model.



*** Tuning ***

We could tune parameters to see how the results change. Here, I increase the number of trees and also perform a 5 fold cross validation.


```{r}

set.seed(1)
boost2<-gbm(Loan_Status~., data=train_loan[,-1],distribution = "gaussian",n.trees = 2000,cv.folds = 5)
sqrt(min(boost2$cv.error))
summary(boost2)
```


We can see the number of relative influence of each variable change and also the RMSE change to 0.3900092, lower than our previous model.


The partial Dependence Plots tell us the relationship and dependence of the variables X with the Response variable Y.

```{r}
#partial dependence plots
plot(boost2,i="LoanAmount")
plot(boost2,i="Income")

```
The first plot shows loanAmount is negatively correlated with the response Loan_Status before 300K. The second plot indicate income is positively correlated with Loan_Status when it less than $20,000.

```{r}
# plot loss function as a result of n trees added to the ensemble
gbm.perf(boost2, method = "cv")
gbm.perf(boost2,method="OOB")

```

The plots indicating the optimum number of trees based on the technique we used. Green line indicates Error on test and the blue dotted line points the optimum number of iterations.We can observe that the beyond a certain a point (55 iterations for the “cv” method and 40 for the "OOB" method), the error on the test data appears to increase because of overfitting. 


We use model 2 to forcast our data.According to the "cv" method, we choose 55 as the number of trees.
```{r}
boostPre<-predict.gbm(boost2,test_loan, n.trees =55)
boostPre<-ifelse(boostPre<1.5,1,2)
boostPre<-factor(boostPre)
test_loan$Loan_Status<-factor(test_loan$Loan_Status)

boost.cm <- confusionMatrix(boostPre, test_loan$Loan_Status)
boost.cm
```

According to the Confusion matrix, Our model accuracy is 0.8293. 


***

#### **Part 5: Model Performance**


***


**Model performance: please compare the models you settled on for problem # 2 – 4.Comment on their relative performance. Which one would you prefer the most? Why?(20 points)**


```{r}
temp <- data.frame(dtree.cm_train$overall, 
                   forest.cm_train$overall,
                   boost.cm$overall) %>%
  t() %>%
  data.frame() %>%
  dplyr::select(Accuracy) %>%
  mutate(`Classification Error Rate` = 1-Accuracy)
```

```{r}
eval <- data.frame(dtree.cm_train$byClass, 
                   forest.cm_train$byClass,
                   boost.cm$byClass)
eval <- data.frame(t(eval)) %>%
  cbind(temp) %>%
  mutate(eval = c("Decision Tree", "Random Forest", "Gradient Boosting")) 
```

```{r}
eval <- dplyr::select(eval, Accuracy, `Classification Error Rate`, Sensitivity, Specificity, Precision, Recall, F1)
rownames(eval) = c("Decision Tree",  "Random Forest", "Gradient Boosting")
t_eval <- t(eval)
colnames(t_eval) <- rownames(eval)
rownames(t_eval) <- colnames(eval)
knitr::kable(t_eval)
```










