---
title: "Data 622 - Final Project (Group 2)"
author: "Mengqin Cai, Zhi Ying Chen, Donny Lofland, Zach Alexander"
date: "5/7/2021"
output: 
  html_document:
    theme: cerulean
    highlight: pygments
    toc: true
    toc_float: true
    code_folding: hide
  pdf_document:
    extra_dependencies: ["geometry", "multicol", "multirow", "xcolor"]
---

Source Code: [https://github.com/djlofland/DATA622_S2021_Group2/tree/master/FinalProject](https://github.com/djlofland/DATA622_S2021_Group2/tree/master/FinalProject)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(readxl)           # Y Read Excel Data
require(naniar)
library(skimr)
require(VIM)
library(imputeTS)
library(doParallel)   # Leverage multiple cores for ML
require(MASS)
require(gbm)
require(xgboost)
library(forecast)
require(rpart)
require(rpart.plot)
require(randomForest)
require(caret)
require(pROC)
library(tidyverse)        # TIDY packages
library(dplyr)   # dplyr has to be loaded before tidyr
library(tidyr)
require(ggplot2)
require(GGally)
require(tidyr)
require(Amelia)
require(psych)
require(class)
require(tree)
require(knitr)
require(kableExtra)
require(mice)
library(corrplot)
library(RColorBrewer)
library(broom)
library(cluster)
library(NbClust)
library(dendextend)
library(e1071)
library(readr)
library(MASS)
library(rpart)
library(caTools)
library(class)
library(Hmisc)
library(randomForest)
library(scales)
library(cluster)
library(DataExplorer)
library(readr)
library(ROCR)
library(PerformanceAnalytics)
library(gbm)
library(ggcorrplot)
library(caTools)
library(naivebayes)
library(ISLR)
library(glmnet)
library(Hmisc)
library(funModeling)
library(klaR)
library(scales)
library(factoextra)
library(ClustOfVar)
```


***

## **Load Data & EDA**

***

### Information about the dataset¶

Before loading our dataset, and discussing our data exploration process, I'll quickly summarize the dataset that we'll be using for our machine learning analysis. The dataset is part of UCI's Cleveland Heart Disease data file, which consists of 303 individuals and various health-related metrics. The dataset we'll be using includes 14 attributes retrieved from patient tests and can be used for machine learning and classification analysis. Ultimately, these attributes will be examined to determine their effectiveness at predicting whether or not a particular patient has Heart Disease.  You can find more information about these 14 attributes below:

**Data Dictionary:**  

* `age`: age in years  
* `sex`: sex (1 = male; 0 = female)  
* `cp`: chest pain type  
    + `Value 1: typical angina  
    + Value 2: atypical angina  
    + Value 3: non-anginal pain  
    + Value 4: asymptomatic  
* `trestbps`: resting blood pressure (in mm Hg on admission to the hospital)  
* `chol`: serum cholestoral in mg/dl  
* `fbs`: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)  
* `restecg`: resting electrocardiographic results  
    + Value 0: normal  
    + Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)  
    + Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria  
* `thalach`: maximum heart rate achieved  
* `exang`: exercise induced angina (1 = yes; 0 = no)  
* `oldpeak`: ST depression induced by exercise relative to rest  
* `slope`: the slope of the peak exercise ST segment  
    + Value 1: upsloping  
    + Value 2: flat  
    + Value 3: downsloping  
* `ca`: number of major vessels (0-3) colored by flourosopy  
* `thal`: 3 = normal; 6 = fixed defect; 7 = reversable defect  
* `target`: diagnosis of heart disease (angiographic disease status)  
    + Value 0: < 50% diameter narrowing  
    + Value 1: > 50% diameter narrowing  
    
Given this context, we are now ready to load our data into R:  

```{r}
df <- read.csv("https://raw.githubusercontent.com/djlofland/DATA622_S2021_Group2/main/FinalProject/data/heart.csv", header = TRUE)
df <- df %>% rename(age = "ï..age")
```

### Exploratory Data Analysis  

First, we can check the shape of the dataset. It looks like there are 303 rows and 14 columns in our imported data.

```{r}
dim(df)
```
### Missing values in the dataset

We can also check to see if there is missing data.

```{r, warning=FALSE}
plot_missing(df)
```

As we can see above, there aren't any missing values across our dataset, which will make it easier for us as we start to prepare our dataset for machine learning analysis. Next, we'll look at a full summary of our features, including rudimentary distributions of each of our continuous variables:    

```{r}
skimr::skim(df)
```

```{r}
df_viz <- df %>% mutate(sex = if_else(sex == 1, "Male", "Female"),
         fbs = if_else(fbs == 1, ">120", "<=120"),
         exang = if_else(exang == 1, "Yes" ,"No"),
         cp = if_else(cp == 1, "Atypical Angina",
                      if_else(cp == 2, "Non-anginal pain", "Asymptomatic")),
         restecg = if_else(restecg == 0, "Normal",
                           if_else(restecg == 1, "Abnormality", "probable or definite")),
         slope = as.factor(slope),
         ca = as.factor(ca),
         thal = as.factor(thal),
         target = if_else(target == 1, "Has heart disease", "Does not have heart disease")
         ) %>% 
  mutate_if(is.character, as.factor) %>% 
  dplyr::select(target, sex, fbs, exang, cp, restecg, slope, ca, thal, everything())
```


Interestingly, we can also see that there's a pretty even split between those that have a presence and absence of heart disease:  

```{r, message=FALSE, warning=FALSE}
# Bar plot for target (Heart disease) 
ggplot(df_viz, aes(x=target, fill=target)) + 
  geom_bar() +
  xlab("Heart Disease") +
  ylab("Count") +
  ggtitle("Analysis of Presence and Absence of Heart Disease") +
  scale_fill_discrete(name = "Heart Disease", labels = c("Absence", "Presence"))
```
```{r}
prop.table(table(df_viz$target))
```

We can see that there is roughly a 50/50 split in this dataset of those that do have heart disease from those that do not have heart disease. This will come in handy later when we set up our machine learning analyses.  

Additionally, when looking at the summary distributions, we can see that our patient population skews above 50 years, with an average age of individuals in this dataset at about 54 years old. The oldest individual is 77 years old, and the youngest is 29. We can take a look at the age frequency counts here:


```{r}
# Counting the frequency of the values of the age
hist(df_viz$age,labels=TRUE,main="Histogram of Age",xlab="Age Class",ylab="Frequency",col="steelblue")
```

```{r}
df_viz %>%
  ggplot(aes(x=age,y=chol,size=chol, color=target))+
  geom_point(alpha=0.7)+xlab("Age") +
  ylab("Cholestoral")
```

Interestingly from the plot above, when we take a look at the relationship between a patient's age, cholesterol and whether or not they have heart disease, we can see that those with positive diagnoses tend to be clustered between the ages of 40 and 70 years and relatively high cholesterol levels.  

We can explore this further by taking a look at these in percentages:  

```{r}
df %>% 
  group_by(target) %>% 
  summarise(mean=mean(trestbps))
```


### Feature Plots

For the continuous variables, we can examine the distributions, broken out by the target variable.  

```{r}
cor_heart <- cor(df[,c(4,5,8,10,14)])
cor_heart

corrplot(cor_heart, method = "ellipse", type="upper",)
ggcorrplot(cor_heart,lab = T)
ggcorr(cor_heart, label = T, label_round = 2)
```

[TODO: More explanation of these corr plots]

### Dummy Columns

We’ll need to dummy code our categorical variables.  This process will create new columns for each value and assign a 0 or 1.  Note that dummy encoding typically drops one value which becomes the baseline.  So if we have a categorical feature with five unique values, we will have four columns.  If all columns are 0, that represents the reference value.  This helps reduce the number of necessary columns.  With dummy columns in place, we need to remove our original variables from this dataset. 

```{r}
# dummy encode our categorical features
df_dummy <- dummyVars(~ 0 + ., drop2nd=TRUE, data = df_viz)
df_dummy <- data.frame(predict(df_dummy, newdata = df_viz))
```


## Principle Component Analysis  

Principal Component Analysis (PCA) is one of the most commonly used unsupervised machine learning algorithms across a variety of applications: exploratory data analysis, dimensionality reduction and information compression. It is a useful technique for exploratory data analysis, allowing us to better visualize the variation present in a dataset with many variables. For our particular use case here, it appears that many of the questionnaire variables fall on likert scales, which when prepared for analysis are extended to dummy variables. This creates many additional features and can make analysis more difficult due to an increased number of dimensions. Therefore, utilizing PCA to reduce the number of dimensions on our entired dataset and measure the amount of variance explained is beneficial. In order to do this, we’ll use the prcomp() function:

```{r}
df_dummy.pca <- prcomp(df_dummy, center = TRUE,scale. = TRUE)
summary(df_dummy.pca)
```

```{r}
fviz_eig(df_dummy.pca)
```

```{r}
#compute standard deviation of each principal component
std_dev <- df_dummy.pca$sdev
#compute variance
pr_var <- std_dev^2
prop_varex <- pr_var/sum(pr_var)
round(prop_varex[1:10], 2)
```

The first principal component in our example therefore explains 20% of the variability, and the second principal component explains 8%. Together, the first ten principal components explain 69% of the variability. And we proceed to plot the PVE and cumulative PVE to provide us our scree plots as we did earlier. 

```{r}
#scree plot
plot(prop_varex, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")
#cumulative scree plot
plot(cumsum(prop_varex), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")
```

```{r}
library("FactoMineR")
library("factoextra")

eig.val <- get_eigenvalue(df_dummy.pca)
eig.val
```

```{r}
res.pca <- PCA(df_dummy, scale.unit = TRUE, ncp = 5, graph = FALSE)
var <- get_pca_var(res.pca)
```

```{r}
corrplot(var$cos2, is.corr=FALSE)
```


```{r}
corrplot(var$contrib, is.corr=FALSE)
```

```{r}
fviz_contrib(df_dummy.pca, choice = "var", axes = 1, top = 15)
fviz_contrib(df_dummy.pca, choice = "var", axes = 2, top = 15)
```

```{r}
fviz_pca_var(df_dummy.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```


[TODO: Need more explanation]


## Splitting the Dataset into Training and Testing  
Our first step will be to separate the data into training and test dataset. This way, we can test the accuracy by using our holdout test set later on. We decided to perform a conventional 80/20 training to testing data split on our original dataframe.
```{r}
df$sex<-as.factor(df$sex)
levels(df$sex)<-c("Female","Male")

df$cp<-as.factor(df$cp)
levels(df$cp)<-c("typical","atypical","non-anginal","asymptomatic")

df$fbs<-as.factor(df$fbs)
levels(df$fbs)<-c("False","True")

df$restecg<-as.factor(df$restecg)
levels(df$restecg)<-c("normal","stt","hypertrophy")

df$exang<-as.factor(df$exang)
levels(df$exang)<-c("No","Yes")

df$slope<-as.factor(df$slope)
levels(df$slope)<-c("upsloping","flat","downsloping")

df$ca<-as.factor(df$ca)

df$thal<-as.factor(df$thal)

df$target<-as.factor(df$target)
levels(df$target)<-c("No", "Yes")
```


```{r}
str(df)
```


```{r}
set.seed(123)
df2      <- data.frame(df)
index    <- createDataPartition(df2$target, p=0.8, list = FALSE)
df_train <- df2[index,]
df_test  <- df2[-index,]
```


## Logistic Regression Analysis  

Logistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability.



[TODO: Not sure if we want to run a basic logistic regression on this, but could be a good start]



## Support Vector Machine (SVM) Analysis  

In machine learning, Support vector machine (SVM) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. It is mostly used in classification problems. In this algorithm, each data item is plotted as a point in n-dimensional space (where n is number of features), with the value of each feature being the value of a particular coordinate. Then, classification is performed by finding the hyper-plane that best differentiates the two classes.

In addition to performing linear classification, SVMs can efficiently perform a non-linear classification, implicitly mapping their inputs into high-dimensional feature spaces.


[TODO: I started the SVM model, but ran into an error when making predicitions. Unfortunately I don't have time to diagnose right now, but wanted to keep it in there in case it's helpful]

```{r}
#x <- subset(df_test, select = -target)
#y <- df_test$target
#df.svm=svm(x, y)
#print(df.svm)

```


## Random Forest Analysis

Random forest is a Supervised Learning algorithm which uses ensemble learning method for classification and regression. It is a bagging technique and not a boosting technique. The trees in random forests are run in parallel. There is no interaction between these trees while building the trees. It operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.

```{r}
fit.forest <- randomForest(target ~ ., data = df_train)

# display model details
fit.forest

plot(fit.forest)
```

Red line represents MCR of class not having heart diseases, green line represents MCR of class having heart diseases and black line represents overall MCR or OOB error. Overall error rate is what we are interested in which seems considerably good.

```{r}
# show which feature were most important
importance(fit.forest)

varImpPlot(fit.forest,type=2)
```

After fitting our model, we can now use it to create predictions on our holdout test set to evaluate its performance. 
```{r}
forest.pred <- predict(fit.forest, newdata = df_test, type = "class")

forest.cm_train <- confusionMatrix(forest.pred, df_test$target)

forest.cm_train
```

The accuracy is 0.8167. The test result is not bad.

[TODO: Need more explanation]


## Kmeans Analysis  

K-means clustering is one of the simplest and popular unsupervised machine learning algorithms that is part of a much deep pool of data techniques and operations in the realm of Data Science. It is the fastest and most efficient algorithm to categorize data points into groups even when very little information is available about data.
```{r}
fviz_nbclust(df_dummy, kmeans, method = "wss") +
  geom_vline(xintercept = 5, linetype = 2) + # add line for better visualization
  labs(subtitle = "Elbow method") # add subtitle
```

We can see above, that it is approximately 5. More clusters do not improve within segment variability.  Therefore, we’ll perform our initial K-Means model with $k=5$.

```{r}
set.seed(42)
km_res <- kmeans(df_dummy, centers = 5, nstart = 20)
summary(km_res)
sil <- silhouette(km_res$cluster, dist(df_dummy))
fviz_silhouette(sil)
```

```{r}
fviz_cluster(km_res, df_dummy, ellipse.type = "norm")
```

To provide some context around the clusters that were generated by our algorithm, we decided to isolate the 5 clusters we created, and subjected them to our `df_dummy` dataset, grouping and computing the means for each one of our features, to see if any noticeable trends start to arise. 

```{r}
k_means_analysis_df <- df_dummy %>%
  mutate(Cluster = km_res$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")
```

```{r}
k_means_analysis_df <- k_means_analysis_df %>% select('Cluster', 'age', 'trestbps', 'chol', 'thalach', 'oldpeak')
kable(k_means_analysis_df) %>% 
  kable_styling(bootstrap_options = "responsive")
```


[Zach Note: just adding these in here since these were a few of the findings I discovered when running this data in python. We can take these out if they aren't helpful, but thought I'd leave them here in case others find similar trends or would like to expand on these.]

## Conclusions  

In the end, we worked through a fair bit of analysis on this heart disease dataset. From this, we've come up with a few interesting conclusions:

Many of the attributes that were available in the Cleveland Heart Disease dataset, and the individuals in this dataset, matched many of the underlying factors that can lead to heart disease. For instance, many that did indeed have heart disease exhibited higher resting blood pressure, higher cholesterol, and were older, on average than those that did not have heart disease.

Additionally, a large percentage of individuals that exhibited exercise-induced angina (chest pain) had heart disease (roughly 75% of the individuals in the dataset). This is something worth noting, as it may help physicians with diagnoses and helping to identify factors that could lead to this type of disease.

It was also very interesting to run some machine learning algorithms on this dataset. Support Vector Machines (SVM) and Random Forest both provided pretty good classification outputs for predicting whether or not an individual has heart disease. Implementing more of these types of models in the future may be valuable to help physicians make proper decisions and can aid in diagnoses.

Finally, the SVM model that I used for this dataset appeared to be slightly more effective when it was implemented on the test dataset. Although the Random Forest model did extraordinarily well on the training dataset (accuracy around 99%), when extending this model to the testing dataset it seemed to yield a higher number of false negatives and false positives thant he SVM model. If this model were to be used in real world scenarios, it could lead to catastrophic consequences if this were the only decision-making tool -- a relatively large percentage of patients could be wrongfully diagnosed with heart disease, and others would be wrongfully told that they do not have heart disease when they actually do.
