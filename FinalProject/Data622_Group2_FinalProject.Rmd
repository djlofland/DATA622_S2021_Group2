---
title: "Data 622 - Final Project (Group 2)"
subtitle: "Predicting Heart Disease"
author: "Mengqin Cai, Zhi Ying Chen, Donny Lofland, Zach Alexander"
date: "5/19/2021"
output: 
  html_document:
    theme: cerulean
    highlight: pygments
    toc: true
    toc_float: true
    code_folding: hide
  pdf_document:
    extra_dependencies: ["geometry", "multicol", "multirow", "xcolor"]
---

Source Code: [https://github.com/djlofland/DATA622_S2021_Group2/tree/master/FinalProject](https://github.com/djlofland/DATA622_S2021_Group2/tree/master/FinalProject)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(readxl)           # Y Read Excel Data
require(naniar)
library(skimr)
require(VIM)
library(imputeTS)
library(doParallel)   # Leverage multiple cores for ML
require(MASS)
require(gbm)
require(xgboost)
library(forecast)
require(rpart)
require(rpart.plot)
require(randomForest)
require(caret)
require(pROC)
library(tidyverse)        # TIDY packages
library(dplyr)   # dplyr has to be loaded before tidyr
library(tidyr)
require(ggplot2)
require(GGally)
require(tidyr)
require(Amelia)
require(psych)
require(class)
require(tree)
require(knitr)
require(kableExtra)
require(mice)
library(corrplot)
library(RColorBrewer)
library(broom)
library(cluster)
library(NbClust)
library(dendextend)
library(e1071)
library(readr)
library(MASS)
library(rpart)
library(caTools)
library(class)
library(Hmisc)
library(randomForest)
library(scales)
library(cluster)
library(DataExplorer)
library(readr)
library(ROCR)
library(PerformanceAnalytics)
library(gbm)
library(ggcorrplot)
library(caTools)
library(naivebayes)
library(ISLR)
library(glmnet)
library(Hmisc)
library(funModeling)
library(klaR)
library(scales)
library(factoextra)
library(ClustOfVar)
```

***

## **Load Data & EDA**

***

### Information about the dataset

Before loading our dataset, and discussing our data exploration process, We'll quickly summarize the dataset that we'll be using for our machine learning analysis. The dataset is part of **UCI's Cleveland Heart Disease** data file, which consists of 303 individuals and various health-related metrics. The dataset we'll be using includes 14 attributes retrieved from patient tests and can be used for machine learning and classification analysis. Ultimately, these attributes will be examined to determine their effectiveness at predicting whether or not a particular patient has Heart Disease.  You can find more information about these 14 attributes below:

**Data Dictionary:**  

* `age`: age in years  
* `sex`: sex (`1` = male; `0` = female)  
* `cp`: chest pain type  
    + Value `1`: typical angina  
    + Value `2`: atypical angina  
    + Value `3`: non-anginal pain  
    + Value `4`: asymptomatic  
* `trestbps`: resting blood pressure (in mm Hg on admission to the hospital)  
* `chol`: serum cholesterol in mg/dl  
* `fbs`: (fasting blood sugar > 120 mg/dl) (`1` = true; `0` = false)  
* `restecg`: resting electrocardiographic results  
    + Value `0`: normal  
    + Value `1`: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)  
    + Value `2`: showing probable or definite left ventricular hypertrophy by Estes' criteria  
* `thalach`: maximum heart rate achieved  
* `exang`: exercise induced angina (`1` = yes; `0` = no)  
* `oldpeak`: ST depression induced by exercise relative to rest  
* `slope`: the slope of the peak exercise ST segment  
    + Value `0`: upsloping  
    + Value `1`: flat  
    + Value `2`: downsloping  
* `ca`: number of major vessels (0-4) colored by fluoroscopy  
* `thal`: `3` = normal; `6` = fixed defect; `7` = reversible defect  
* `target`: diagnosis of heart disease (angiographic disease status)  
    + Value `0`: < 50% diameter narrowing  
    + Value `1`: > 50% diameter narrowing  
    
### Exploratory Data Analysis  

First, we can check the shape of the dataset. It looks like there are 303 rows and 14 columns in our imported data.  We also show the first few rows of data just to sanity check that things look correct.

```{r}
df <- read.csv("./data/heart.csv", header = TRUE)

dim(df)
```

```{r}
kable(head(df)) 
```

### Missing values in the dataset

Missing values can be problematic if present. We can also check to see if there is missing data.

```{r, warning=FALSE}
plot_missing(df)
```

As we can see above, there aren't any missing values across our dataset, which will make it easier for us as we start to prepare our dataset for machine learning analysis. 

As a next steps, let's convert known categorical features into `factor` data type, then show a summary of our dataframe.

```{r}
df_viz <- df %>% 
  mutate(sex = if_else(sex == 1, "Male", "Female"),
         fbs = ordered(fbs, levels=c(0, 1), labels=c("<=120", ">120")),
         exang = if_else(exang == 1, "Yes" ,"No"),
         cp = if_else(cp == 1, "Atypical Angina",
                      if_else(cp == 2, "Non-anginal pain", "Asymptomatic")),
         restecg = if_else(restecg == 0, "Normal",
                           if_else(restecg == 1, "Abnormality", "probable or definite")),
         slope = ordered(slope, 
                         levels=c(2, 1, 0), 
                         labels=c('down', 'flat', 'up')),
         ca = ordered(ca, 
                      levels=c(0, 1, 2, 3, 4), 
                      labels=c(0, 1, 2, 3, 4)),
         thal = as.factor(thal),
         target = if_else(target == 1, "Has heart disease", "Does not have heart disease")) %>% 
  mutate_if(is.character, as.factor) %>% 
  dplyr::select(target, sex, fbs, exang, cp, restecg, slope, ca, thal, everything())
```

Next, we'll look at a full summary of our features, including rudimentary distributions of each of our continuous variables:    

```{r}
skimr::skim(df_viz)
```

Looking at our `target`, there is a reasonable class balance between the presence and absence of heart disease.  However, we do see value class imbalance when looking at several of the factor features: `fbs`, `exang`, `cp`, `restecg`, `slop`, `ca`, and `thal`.  

While `age` looks normally distributed, there is apparent right-skew in `trestbps`, `chol`, and `oldpeak`.  `thalach` is left skewed.  Our continuous features are also on different scales.  We will want to transform and normalize these ahead of modeling. 

Let's visualize our `target` distribution.

```{r, message=FALSE, warning=FALSE}
# Bar plot for target (Heart disease) 
ggplot(df_viz, aes(x=target, fill=target)) + 
  geom_bar() +
  xlab("Heart Disease") +
  ylab("Count") +
  ggtitle("Analysis of Presence and Absence of Heart Disease") +
  scale_fill_discrete(name = "Heart Disease", labels = c("Absence", "Presence"))
```

```{r}
prop.table(table(df_viz$target))
```

We can see that there is roughly a 50/50 split in this dataset of those that do have heart disease from those that do not have heart disease. This will come in handy later when we set up our machine learning analyses.  

Additionally, when looking at the summary distributions, we can see that our patient population skews above 50 years, with an average age of individuals in this dataset at about 54 years old. The oldest individual is 77 years old, and the youngest is 29. We can take a look at the age frequency counts here:

```{r}
# Counting the frequency of the values of the age
hist(df_viz$age, labels=TRUE, main="Histogram of Age", xlab="Age Class", 
     ylab="Frequency", col="steelblue")
```

```{r}
df_viz %>%
  ggplot(aes(x=age, y=chol, size=chol, color=target)) +
  geom_density2d() + 
  geom_point(alpha=0.4) + 
  xlab("Age") +
  ylab("Cholesterol")
```

Interestingly from the plot above, when we take a look at the relationship between a patient's age, cholesterol and whether or not they have heart disease, we can see that those with positive diagnoses tend to be clustered between the ages of 40 and 70 years and relatively high cholesterol levels.  

We can explore this further by taking a look at these in percentages:  

```{r}
df %>% 
  group_by(target) %>% 
  summarise(mean=mean(trestbps))
```

### Feature Plots

Next, we explore the relationships between each feature and the target.  For our continuous features, we do a simple `featurePlot` from the `caret` package which also lets us see how pairs of features relate with the target.

```{r fig.height=5, fig.width=5}
featurePlot(x=df_viz[,c(10, 11, 12, 13, 14)], y=df_viz[, 1], 
            plot = "ellipse", 
            auto.key = list(columns = 2))
```

Here we see some patterns emerge, for example, `oldpeak` tends to cluster towards lower values across all other features in the heart disease group.  Also `thalach` tends towards higher values across each other features in the heart disease group. `chol` and `thalach` together have a tighter clustering and separation between our target classes.

For the continuous variables, we can also examine the correlations, broken out by the target variable.  

```{r message=FALSE, warning=FALSE}
wo_hd <- df_viz %>% 
  filter(target == 'Does not have heart disease') %>% 
  select(age, trestbps, chol, thalach, oldpeak)

cor_heart_wo <- cor(wo_hd)

ggcorr(cor_heart_wo, label = T, label_round = 2) +
  scale_fill_gradient2(low='#ff0000', mid='#ffffff', high='#00ff00', 
                        midpoint=0, guide = "colourbar", aesthetics = "fill") +
  ggtitle('Correlation Plot: Patients without Heart Disease')

wi_hd <- df_viz %>% 
  filter(target == 'Has heart disease') %>% 
  select(age, trestbps, chol, thalach, oldpeak)

cor_heart_wi <- cor(wi_hd)

ggcorr(cor_heart_wi, label = T, label_round = 2) +
  scale_fill_gradient2(low='#ff0000', mid='#ffffff', high='#00ff00', 
                        midpoint=0, guide = "colourbar", aesthetics = "fill") +
  ggtitle('Correlation Plot: Patients with Heart Disease')

cor_heart_delta = cor_heart_wi - cor_heart_wo

delta_df <- as.data.frame(cor_heart_delta)

delta_dfl <- delta_df %>% 
  rownames_to_column(var="X") %>%
  pivot_longer(cols=-c(X), names_to='Y')
  
ggplot(delta_dfl, aes(X, Y, fill=value)) +
  geom_tile() + 
  coord_flip() +
  scale_fill_gradient2(low='#ff0000', mid='#ffffff', high='#00ff00', 
                        midpoint=0, guide = "colourbar", aesthetics = "fill") +
  ggtitle('Difference between Correlograms: With Disease - Without Disease')
```

We separate out the correleograms based on `target` class.  The first chart show the pairwise correlations between continuous variable in our Healthy patients.  The second chart shows the pairwise correlations in patients with heart disease.  The third chart sows the difference to help draw attention to how heart disease patients differ from healthy patients.  For example, `age` vs. `thalach` correlation in healthy patients is `-0.55`.  In heart diseased patients, the correlation is `-0.94` ... there is a much stronger negative correlation between these features in our diseased patients.  The third chart show a bright red filled for `age` vs. `thalach` to draw attention that this correlation became more negative.  As a counter example, the `age` vs. `cholesterol` correlation is higher in diseased patients vs healthy (0.23 > -0.12).  This increase in correlation shows as a light green tile in the Difference plot (third).

We can use bar charts to visualize how our categorical features are distribute across the two target classes.

```{r fig.width=8}
long_df <- df_viz %>% 
  select(c(target:thal)) %>%
  mutate(fbs = factor(as.character(fbs)),
         ca = factor(as.character(ca)),
         thal = factor(as.character(thal)),
         slope = factor(as.character(slope))) %>%
  pivot_longer(cols=-c(target), names_to='kpi')

ggplot(long_df, aes(x=value, fill=target)) + 
  geom_bar() + 
  facet_wrap(~kpi, scales="free_x") + 
  ggtitle('Comparing Categorical Features and Target')
```

A few notable points:

* Looking as `sex`, we now see that of the patients in this study, a far higher portion of `female` presented with heat disease compared with the `males`.  
* `exang` of `No` is more common with having heart disease. 
* As we would expect, `cp` = `Asymptomatic` is far more correlated with being healthy.

### Dummy Columns

We'll need to dummy code our categorical variables.  This process will create new columns for each value and assign a 0 or 1.  Note that dummy encoding typically drops one value which becomes the baseline.  So if we have a categorical feature with five unique values, we will have four columns.  If all columns are 0, that represents the reference value.  This helps reduce the number of necessary columns.  With dummy columns in place, we need to remove our original variables from this dataset. 

```{r}
# dummy encode our categorical features
df_dummy <- dummyVars(~ ., fullRank=T, drop2nd=T, data=df_viz)
df_dummy <- data.frame(predict(df_dummy, newdata=df_viz))

df_dummy <- df_dummy %>%
  mutate(target = as.factor(target.Has.heart.disease)) %>%
  select(-target.Has.heart.disease)
```

Note: the `dummyVars()` function from `caret` also automatically applies a scale and center transform on all our continuous features.  While we could probably choose custom transformations that improve performance (e.g. using `BoxCox.lambda` per feature), the distributions we saw above didn't look that skewed.  The default `caret` transform is probably sufficient.  

## Splitting the Dataset into Training and Testing

Our first step will be to separate the data into training and test dataset. This way, we can test the accuracy by using our holdout test set later on. We decided to perform a conventional 80/20 training to testing data split on dummied dataset.

```{r}
set.seed(123)

index    <- createDataPartition(df_dummy$target, p=0.8, list = FALSE)

df_train <- df_dummy[index,]
df_test  <- df_dummy[-index,]
```

With our dataset prepared and training/test split in place, we re ready to proceed with trying different machine learning models to predict heart disease given our feature.

## Supervised Machine Learning

### Random Forest Analysis

Random forest is a Supervised Learning algorithm which uses ensemble learning method for classification and regression. It is a bagging technique and not a boosting technique. The trees in random forests are run in parallel. There is no interaction between these trees while building the trees. It operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.

```{r}
set.seed(123)
fit.forest <- randomForest(target ~ ., data = df_train, importance=TRUE, ntree=2000)

# display model details
fit.forest

plot(fit.forest)
```

Red line represents MCR of class not having heart diseases, green line represents MCR of class having heart diseases and black line represents overall MCR or OOB error. Overall error rate is what we are interested in which seems considerably good.

```{r}
# show which feature were most important
importance(fit.forest)

varImpPlot(fit.forest,type=2)
```

After fitting our model, we can now use it to create predictions on our holdout test set to evaluate its performance. 

```{r}
rf.pred <- predict(fit.forest, newdata=df_test, type = "class")
(forest.cm_train <- confusionMatrix(rf.pred, df_test$target))

rf.perf <- c(forest.cm_train$overall[1], 
             forest.cm_train$byClass[1], 
             forest.cm_train$byClass[2], 
             forest.cm_train$byClass[5], 
             forest.cm_train$byClass[6], 
             forest.cm_train$byClass[7])

```

The accuracy is 0.85. The test result is not bad. [TODO: Discuss sensitivity and specificity]

[TODO: Need more explanation]


### Gradient Boosting

```{r}
set.seed(123)

# build gradient boosted model
fit.boost <- gbm(target~., df_train,
             distribution = "gaussian",
             n.trees = 2000,
             cv.folds = 5)

# display model details
fit.boost
```



```{r}
sqrt(min(fit.boost$cv.error))
summary(fit.boost)
```



```{r}
plot(fit.boost, i="chol")
```



```{r}

plot(fit.boost, i="thalach")
```

```{r}
# plot loss function as a result of n trees added to the ensemble
gbm.perf(fit.boost, method = "cv")
```

```{r}

gbm.perf(fit.boost, method = "OOB")

```

The plots indicating the optimum number of trees based on the technique we used. The green line indicates the error on the test data, and the blue dotted line points to the optimum number of iterations. We can observe that beyond a certain point (82 iterations for the cross-validation method and 43 for the OOB method), the test data error appears to increase because of overfitting.

According to the cross-validation method, we choose 42 as the number of trees to predict the test set.

```{r}
boostPre <- predict.gbm(fit.boost, 
                        df_test, 
                        n.trees = 42)
boostPre <- ifelse(boostPre < 1.5, 0, 1)

gbm.pred <- as.factor(boostPre)

(boost.cm <- confusionMatrix(gbm.pred, df_test$target))

gbm.perf <- c(boost.cm$overall[1],
              boost.cm$byClass[1],
              boost.cm$byClass[2],
              boost.cm$byClass[5],
              boost.cm$byClass[6],
              boost.cm$byClass[7])
```

[TODO: Discussion ... accuracy, sensitvity, specificity]

### Support Vector Machine

In machine learning, Support vector machine (SVM) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. It is mostly used in classification problems. In this algorithm, each data item is plotted as a point in n-dimensional space (where n is number of features), with the value of each feature being the value of a particular coordinate. Then, classification is performed by finding the hyper-plane that best differentiates the two classes.

In addition to performing linear classification, SVMs can efficiently perform a non-linear classification, implicitly mapping their inputs into high-dimensional feature spaces.

```{r}
set.seed(123)

fit.svm <- svm(target~., 
               data=df_train, 
               kernel="radial", 
               cost=10, 
               scale = FALSE)
fit.svm
```

```{r}
svm.pred <- predict(fit.svm, 
                    newdata = df_test)

(svm.cm_train <- confusionMatrix(svm.pred, df_test$target))
```

[Discuss: ...]


```{r}

svm.tune <- tune(e1071::svm, target~.,
                 data = df_train,
                 ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100),
                 kernel=c("linear", "polynomial", "radial"),
                 gamma=c(0.5, 1, 2, 3, 4)))

```



```{r}
summary(svm.tune)
```



```{r}
bestmod <- svm.tune$best.model
bestmod

svm.pred2 <- predict(bestmod, newdata = df_test, type = "class")

(svm.cm_train2 <- confusionMatrix(svm.pred2, df_test$target))

svm.perf <- c(svm.cm_train2$overall[1],
              svm.cm_train2$byClass[1],
              svm.cm_train2$byClass[2],
              svm.cm_train2$byClass[5],
              svm.cm_train2$byClass[6],
              svm.cm_train2$byClass[7])
```

According to the Confusion matrix, Our model Accuracy is 0.8333. [TODO: Discuss]

### Neural Network

```{r warning=FALSE}
set.seed(123)


## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(
  .decay = c(0, 0.1, 1),
  .size = c(1:10),
  ## The next option is to use bagging (see the
  ## next chapter) instead of different random
  ## seeds.
  .bag = FALSE)

ctrl <- trainControl(method='cv', number=10)

nnetModel <- train(df_train[, 1:21], df_train[, 22],
  method = "avNNet",
  tuneGrid = nnetGrid,
  trControl = ctrl,
  linout = TRUE,
  trace = FALSE,
  MaxNWts = 243,
  maxit = 500)

# Model results
plot(nnetModel)
summary(nnetModel)
varImp(nnetModel)

nn.pred <- predict(nnetModel, newdata = df_test[, 1:21])

(nn.cm_train <- confusionMatrix(nn.pred, df_test[, 22]))

nn.perf <- c(nn.cm_train$overall[1], 
             nn.cm_train$byClass[1], 
             nn.cm_train$byClass[2], 
             nn.cm_train$byClass[5], 
             nn.cm_train$byClass[6], 
             nn.cm_train$byClass[7])
```


### Multivariate Adaptive Regressive Splines (MARS)

One final model we will try is MARS.  MARS has the advantage that it uses derived features so in effect has a built in dimensionality reduction steps.

```{r warning=FALSE}
set.seed(123)

# Define the candidate models to test
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)

marsModel <- train(df_train[, 1:21], df_train[, 22],
  method = "earth",
  tuneGrid = marsGrid,
  trControl = trainControl(method = "cv"))

# Model results
plot(marsModel)
summary(marsModel)
varImp(marsModel)

mars.pred <- predict(marsModel, newdata = df_test[, 1:21])

mars.cm_train <- confusionMatrix(mars.pred, df_test[, 22])

(mars.perf <- c(mars.cm_train$overall[1], 
                mars.cm_train$byClass[1], 
                mars.cm_train$byClass[2], 
                mars.cm_train$byClass[5], 
                mars.cm_train$byClass[6], 
                mars.cm_train$byClass[7]))

```


### Ensemble Classifier

Lastly, let's build an ensemble classifer that combines all of the models we've trained.  The simplest approach is to predict the class with each model, then take a vote of the 5 models on which class should be selected. The hope is that the models made errors on different rows so by combining, models can help cover for each other where a given model might have struggled. 

```{r}
# Create the function.
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

esm.df <- data.frame(cbind(rf.pred, gbm.pred, svm.pred2, nn.pred, mars.pred))
esm.df$vote <- 0

esm.df <- esm.df %>%
  rowwise() %>%
  mutate(vote = getmode(c(rf.pred, gbm.pred, svm.pred2, nn.pred, mars.pred)))

esm.df <- esm.df %>%
  mutate(vote = factor(vote, levels=c(1,2), labels=c(0,1)))

esm.cm_train <- confusionMatrix(esm.df[['vote']], df_test[, 22])

(esm.perf <- c(esm.cm_train$overall[1], 
               esm.cm_train$byClass[1], 
               esm.cm_train$byClass[2], 
               esm.cm_train$byClass[5], 
               esm.cm_train$byClass[6], 
               esm.cm_train$byClass[7]))
```

See discussion of results in our conclusion.

###  Supervised Model Performance

Here are the tablulated results from all our models, including the Ensemble Model that combines all 5.

```{r}
result <- rbind(rf.perf, gbm.perf, svm.perf, nn.perf, mars.perf, esm.perf)

kable(result)
```
See discussion of performance in our conclusion below.

## Conclusions  

In the end, we worked through a fair bit of analysis on this heart disease dataset. From this, we've come up with a few interesting conclusions:

Many of the attributes that were available in the Cleveland Heart Disease dataset, and the individuals in this dataset, matched many of the underlying factors that can lead to heart disease. For instance, many that did indeed have heart disease exhibited higher resting blood pressure, higher cholesterol, and were older, on average than those that did not have heart disease.

Additionally, a large percentage of individuals that exhibited exercise-induced angina (chest pain) had heart disease (roughly 75% of the individuals in the dataset). This is something worth noting, as it may help physicians with diagnoses and helping to identify factors that could lead to this type of disease.

[TODO: Discuss feature importance across 5 models]

It was also very interesting to run some machine learning algorithms on this dataset. Support Vector Machines (SVM) and Random Forest both provided pretty good classification outputs for predicting whether or not an individual has heart disease. Implementing more of these types of models in the future may be valuable to help physicians make proper decisions and can aid in diagnoses.

We ended up training 5 different models with the data set: Random Forest, Gradient Booste Machine, Support Vector Machine, Neural Network, and Multiple Adaptive Regressinve Splines.  The performance measures that are most important to us are Accuracy and Sensitvity (aka Recall).  Accuracy will tell us our True Positive ans True Negative rate which are important.  When working with health related prediction, errors leading to False Negative are far worse than errors leading to False Positive.  The logic is that if we incorrectly predict someone has having disease (and they get unnecessary followup), that is better than the alternative where we fail to predict and they don't get necessary care.  So, to minimize False Negative, we want to maximize Sensitvity (aka Recall).  We would like to have high Specifcity (or Precision), but that is a lower priority.

Given this context, our GBM had $Accuracy=0.8500$ and $Sensitivity=0.7778$ and our Neural Network came in with $Accuracy=0.8167$ and $Sensitivity=0.8148148$.  However, the overall winner is out Ensemble Model that combines all the individual models.  The Ensemble had $Accuracy=0.8500$ and $Senitvity=0.8148148$ - giving us the best of both.  The reason Ensembles can outperform - and especially when facing unseen data - is that every individual model will have some blindspots and make errors.  When we combine a number of models, then we leverage the group to help catch errors by individual models.  With 5 different models, they vote, best 3 out of 5.  This helps compensate for one-off errors by models.  Note, we tried several combinations of Ensemble, dropping out the 2 lowest performing models (RF and MARS), but while they were individually the lowest performance individually, we we removed them from the emsemble, the ensemble performance decreased.  So apparantly, while RF and MARS didn't do as well overall, they did add value to the group.